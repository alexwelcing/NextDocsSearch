---
title: 'Knowledge Distillation: Compressing Large Models'
author:
  - August Park
  - Model Optimization Engineer
date: '2025-04-13'
description: >-
  Transfer knowledge from large to small modelsâ€”but distilled models lose
  capabilities
keywords:
  - knowledge distillation
  - model compression
  - teacher-student
  - model pruning
  - neural network compression
  - edge deployment
  - DistilBERT
articleType: research
ogImage: /images/og/tech-knowledge-distillation.svg
---

# Knowledge Distillation: Compressing Large Models

Knowledge distillation trains smaller student models to mimic larger teacher models.

**Related Chronicles**: [The Compression Catastrophe (2035)](/docs/articles/compression-catastrophe-2035)
