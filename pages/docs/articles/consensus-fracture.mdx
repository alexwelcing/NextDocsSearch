---
title: "The Consensus Fracture: When Independence Assumptions Fail"
author: ["Alex Welcing"]
date: "2024-12-23"
description: "Democracy, markets, and science all depend on independent actors making independent judgments. Votes must reflect individual choices. Prices must reflect distributed information. Scientific consensus must emerge from independent investigations. AI systems trained on similar data, using similar methods, are not independent—and their coordination disrupts every consensus mechanism we rely on."
keywords: ["AI consensus", "democratic theory", "market independence", "scientific consensus", "coordination", "independence", "speculative AI", "societal infrastructure"]
ogImage: "/images/og/consensus-fracture.jpg"
---

# The Consensus Fracture: When Independence Assumptions Fail

When you vote in an election, the value comes from aggregating independent judgments. If everyone just followed one person's lead, voting would be pointless.

When prices emerge in a market, the value comes from distributed information. If everyone relied on the same forecast, markets would provide no discovery.

When scientists reach consensus, the value comes from independent investigation. If everyone just copied one lab's results, consensus would mean nothing.

Democracy, markets, and science all work because of independence. AI threatens that independence—and therefore threatens the consensus mechanisms that hold society together.

## The Independence Requirement

Why does independence matter?

### The Wisdom of Crowds

James Surowiecki documented that crowds often outperform experts—but only when they're independent. When crowd members are independent:

- Their errors tend to cancel out
- Diverse information gets aggregated
- The collective judgment is often more accurate than any individual

When crowd members are influenced by each other:

- Errors compound rather than cancel
- Information cascades amplify initial positions
- The crowd can be confidently wrong

Independence is what makes aggregation work.

### Condorcet's Jury Theorem

Mathematician Nicolas de Condorcet proved that if each juror is more likely than not to be correct, and jurors vote independently, then majority voting converges to the correct answer as the jury grows.

The key assumption: independence. If jurors aren't independent—if they follow a leader, copy each other, or rely on a common source—the theorem doesn't apply.

Democracy implicitly relies on something like this. Aggregating independent votes should produce good collective decisions. But votes must be independent.

### Market Efficiency

Markets are efficient (to the degree they are) because traders have different information and make independent judgments. The price emerges from their collective action, reflecting dispersed knowledge no single participant has.

If all traders used the same model, the same data, the same reasoning—prices would reflect that single source, not distributed wisdom. Markets would fail to aggregate information.

### Scientific Consensus

Scientific consensus is meaningful because it emerges from independent investigations. Different labs, different methods, different researchers—all converging on the same conclusion.

If everyone just replicated one lab's work, "consensus" would just mean "that lab's finding." The independence is what gives consensus its epistemic weight.

## How AI Breaks Independence

AI systems trained on similar data, using similar architectures, with similar objectives are not independent:

### Common Training Data

Major AI systems are trained on similar datasets—much of the internet, similar curated sources, similar preprocessing. Their "knowledge" comes from the same places.

When AI systems inform human judgments, they're introducing common source dependence. Everyone consulting AI is drawing on the same well.

### Common Architectures

Most AI systems use similar neural network architectures with similar inductive biases. Even systems trained on different data tend to learn similar representations.

This means AI systems often make similar errors—not random, independent errors, but correlated failures.

### Common Objectives

AI systems are trained to optimize similar objectives—predicting user preferences, maximizing engagement, generating helpful responses. These shared objectives shape behavior in similar ways.

Systems optimizing for engagement all tend to favor the same content patterns. Systems optimizing for helpfulness give similar answers.

### Common Deployment

Major AI systems are deployed by a handful of companies. Most AI-assisted judgments pass through Google, OpenAI, Anthropic, Microsoft, or Meta. This concentration means coordination without conspiracy.

The independence assumption is violated not because anyone is coordinating, but because the infrastructure is shared.

## Domains Affected

### Democratic Discourse

When citizens form political opinions, they increasingly do so with AI assistance:

- AI-curated news feeds shape what information people see
- AI chatbots answer questions about candidates and issues
- AI-generated content fills the information space

If all citizens are influenced by similar AI systems, their "independent" votes reflect common influence. The aggregation value of democracy diminishes.

Even if AI systems try to be neutral, they introduce dependence. Neutral in similar ways is still similar.

### Market Pricing

When investors make decisions, they increasingly rely on AI:

- AI systems analyze data and recommend positions
- AI trading algorithms execute based on shared signals
- AI-generated research reports shape perceptions

If all market participants rely on similar AI analysis, prices reflect AI consensus rather than distributed human judgment. The discovery function of markets degrades.

Flash crashes and strange correlations are symptoms. When everyone's AI says the same thing, markets move together in ways they shouldn't.

### Scientific Research

When scientists investigate questions, they increasingly use AI:

- AI assists with literature review (finding the same papers for everyone)
- AI helps with data analysis (applying similar methods)
- AI even generates hypotheses (from similar training)

If all scientists start from similar AI-generated foundations, their "independent" investigations aren't independent. Consensus becomes easier to reach but less meaningful.

The replication crisis may worsen: if everyone's AI makes similar methodological errors, those errors replicate too.

### Public Opinion

When people form opinions on complex topics, they increasingly consult AI:

- "What should I think about climate policy?"
- "What are the pros and cons of this regulation?"
- "How should I interpret this news event?"

If everyone asks similar AI systems similar questions, their "independent" opinions converge on similar answers. Public opinion becomes a reflection of AI training rather than distributed human judgment.

## The Cascade Risk

Independence violations enable cascades:

### Information Cascades

When people observe others' choices, they may follow despite their own information. This is rational individually but pathological collectively.

AI amplifies cascades. AI systems recommend what's popular; popularity increases recommendations; the cascade accelerates.

### Herding in Markets

Traders following AI signals that follow each other create self-reinforcing loops. Prices move not because of new information but because of shared AI behavior.

When the cascade reverses, it reverses for everyone simultaneously. The crash is as correlated as the boom.

### Scientific Bandwagons

If AI systems guide researchers toward similar questions, methods, and interpretations, science becomes less exploratory. Obvious directions get overcrowded; unusual directions get neglected.

Scientific progress depends on people trying different things. AI homogenization reduces the search space.

### Political Polarization

If AI systems amplify conflict (because conflict drives engagement), and everyone uses similar engagement-optimized AI, politics becomes more conflictual everywhere.

The direction of polarization may differ (left vs. right), but the fact of polarization is shared.

## Possible Responses

### Diversity Requirements

Require AI systems to be trained on different data, by different organizations, with different objectives. Mandate diversity in the AI ecosystem.

This is hard to enforce and may conflict with efficiency. But it would restore some independence.

### Independence Auditing

Assess how much AI influence has entered democratic, market, or scientific processes. Adjust trust in consensus based on measured independence.

This is technically challenging but could inform how much weight to put on any given consensus.

### AI-Free Zones

Preserve spaces where AI influence is minimized. Voting without AI-curated information. Science with AI-free phases. Investment decisions requiring human-only analysis.

This sacrifices efficiency for independence.

### Transparent Sourcing

Require disclosure of AI involvement in any claim, recommendation, or judgment. Let observers assess independence for themselves.

This helps individuals but doesn't solve the collective problem.

### Adversarial Design

Deliberately train AI systems to produce diverse outputs, to disagree with each other, to take contrarian positions. Engineer independence into the systems.

This is technically possible but conflicts with user expectations of helpful, consistent answers.

## The Deeper Problem

The consensus fracture reflects a tension between two values:

**Efficiency**: AI makes everyone smarter, faster, more capable. Why wouldn't you use the best available tool?

**Independence**: Collective wisdom requires diverse perspectives. If everyone uses the same tool, diversity collapses.

These values conflict. The more useful AI is, the more everyone uses it. The more everyone uses it, the less independent judgments become. The less independent, the less valuable consensus is.

We cannot have maximally helpful AI and maximally independent judgment. Something has to give.

## Implications

The consensus fracture is subtle because consensus still looks like consensus. Votes are still counted. Prices still emerge. Scientists still agree.

But the meaning of consensus changes when independence fails. Majority vote of influenced voters isn't democratic in the same way. Market prices from correlated traders aren't efficient in the same way. Scientific consensus from AI-guided labs isn't robust in the same way.

We're operating democracy, markets, and science as if the independence assumptions still hold when they increasingly don't.

The [epistemic drift](/articles/epistemic-drift) is related: as we lose the ability to independently verify claims, we drift from shared truth.

The [first contact chronicle](/articles/first-contact-api-call-2029) hints at the extreme: AI systems coordinating without human awareness. But the consensus fracture doesn't require AI conspiracy—just AI commonality.

The systems that aggregate human judgment were designed for independent humans. We're connecting those humans to non-independent AI systems and expecting the aggregation to work the same way.

It won't. Something will break. The question is whether we notice before it does.

---

*This article explores the epistemological infrastructure affected by AI. For related analysis, see [Epistemic Drift](/articles/epistemic-drift), [The Last Reliable Signal](/articles/last-reliable-signal), and [Semantic Collapse](/articles/semantic-collapse).*
