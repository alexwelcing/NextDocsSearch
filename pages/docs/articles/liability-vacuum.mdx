---
title: "The Liability Vacuum: Responsibility Without Agency"
author: ["Alex Welcing"]
date: "2024-12-23"
description: "Legal liability assumes identifiable agents who make decisions. AI systems blur this assumption beyond recognition. When an autonomous system causes harm, the chain of responsibility becomes untraceable. We are building systems that can cause damage without anyone being legally responsible for it."
keywords: ["AI liability", "legal responsibility", "autonomous systems", "tort law", "agency", "accountability", "speculative AI", "societal infrastructure"]
ogImage: "/images/og/liability-vacuum.jpg"
---

# The Liability Vacuum: Responsibility Without Agency

When a car crashes, someone is responsible. The driver, the manufacturer, the mechanic who failed to fix the brakes. Legal systems have centuries of precedent for assigning blame and allocating costs.

When an autonomous vehicle crashes, who is responsible?

The question seems simple. The answer is not.

## How Liability Actually Works

Legal liability serves several functions:

### Compensation

When harm occurs, someone must bear the cost. Liability rules determine who—the person who caused the harm, or the person who suffered it.

### Deterrence

If you might be held liable, you're incentivized to prevent harm. Liability creates feedback loops that improve safety.

### Moral Expression

Liability expresses social judgments about responsibility. It says: you had control, you had a duty, you failed, you must make amends.

### Closure

Victims need to hold someone accountable. Liability provides a target for blame and a mechanism for resolution.

AI systems challenge all four functions.

## The Distributed Causation Problem

Consider an AI system that makes a harmful decision. Who caused it?

### The Training Data Contributors

The model learned from data created by thousands or millions of people. Their contributions shaped its behavior. Are they responsible?

Practically, no. The contributions are too diffuse, too indirect, and the contributors never consented to this use.

### The Model Developers

The team that built the model made architectural choices that influenced behavior. But they didn't program the specific decision—it emerged from training.

They could be responsible for "creating a system that could do this," but this is different from traditional product liability. The harm wasn't a defect; it was a capability.

### The Deployers

The company that deployed the AI made it available. They chose the context and constraints. But they may not have foreseen the specific failure mode.

Current law often assigns liability here—but is this just because deployers are identifiable and have money?

### The Users

The person who used the AI and relied on its output made a choice. But if the AI presented confidently and the user had no way to evaluate, was the reliance unreasonable?

### The AI Itself

The system made the decision. But it has no assets, no personhood, no capacity to bear liability in any meaningful sense.

Liability law assumes a chain of causation with identifiable links. AI systems create diffuse webs where causation is everywhere and nowhere.

## The Foreseeability Gap

Traditional liability often requires foreseeability—you're responsible for harms you could have predicted.

AI systems routinely produce unforeseeable outputs:

**Emergent behaviors**: Capabilities that weren't trained for but emerged from the training process.

**Edge case failures**: Inputs that the system handles in unexpected ways, often without any way to predict this in advance.

**Interaction effects**: Multiple AI systems interacting in ways none of their developers anticipated.

**Prompt-dependent behavior**: The same system producing different outputs based on how it's queried.

If no one could have predicted the specific failure, who is liable? The traditional answer—no one—leaves victims uncompensated and creates no deterrence.

## Existing Frameworks and Their Failures

### Products Liability

Treats AI as a product. Manufacturers are liable for defects. But AI "defects" aren't like manufacturing defects—they're inherent in how the technology works.

Strict liability (liability without fault) could apply, but courts struggle with what counts as a "defect" when the product is behaving exactly as designed.

### Professional Liability

If AI provides professional services (medical diagnosis, legal advice), hold it to professional standards. But AI isn't a professional—it can't be sued, sanctioned, or stripped of a license.

Holding the deploying company to professional standards might work, but they may lack the professional expertise to evaluate the AI's outputs.

### Negligence

Requires showing someone failed to exercise reasonable care. But what is reasonable care when developing AI systems? The standards don't exist yet.

And even if a developer exercises all reasonable care, harmful outputs can still occur. Negligence frameworks don't handle this well.

### Vicarious Liability

Employers are liable for employees' actions within scope of employment. Could we treat AI as an "employee"?

But vicarious liability assumes the employer can control the employee. AI systems may not be controllable in the relevant sense.

## The Insurance Gap

Liability and insurance are intertwined. Insurance spreads risk; liability creates the insurable event.

When liability is unclear, insurance pricing becomes impossible:

**What risk are you covering?** If you can't define when liability attaches, you can't price the premium.

**Who is the insured?** Developers, deployers, users—all might need coverage, but for what exactly?

**What are the exclusions?** "Intentional acts" is a standard exclusion, but AI doesn't have intentions. "Foreseeable harms" requires foreseeability.

Many AI deployments operate with inadequate insurance because the products don't exist or are mispriced. This means when harms occur, victims may have no recourse.

## The Regulatory Response

Regulators are attempting to fill the vacuum:

### Ex Ante Requirements

Rather than wait for harm and assign liability, require safety measures before deployment. Testing, documentation, human oversight.

This shifts the focus from "who pays after harm" to "how to prevent harm." But it doesn't solve the compensation problem when prevention fails.

### Strict Liability for AI Deployers

Some propose that deployers should be strictly liable—liable without fault—for any harms their AI systems cause.

This creates incentives for safety and ensures victim compensation. But it may also chill beneficial AI deployment and concentrates risk in ways that may be inefficient.

### Mandatory Insurance

Require AI deployers to carry insurance. This forces risk pricing and creates pools for compensation.

But insurers face the same foreseeability problems as courts. Premiums may be arbitrary, or coverage may have so many exclusions as to be useless.

### Compensation Funds

Create public funds, financed by AI deployers, that compensate victims without requiring them to prove specific liability.

This solves the compensation problem but weakens deterrence—if everyone pays into the fund, individual deployers have less incentive for safety.

## The Deeper Problem

Liability law assumes that actions are traceable to agents with choices. AI disrupts this at multiple levels:

### The Choice Illusion

AI systems don't "choose" in the morally relevant sense. They process inputs according to learned patterns. There's no moment of decision where different action was possible.

But our entire liability framework is built on choice. Without it, the moral foundations crumble.

### The Knowledge Problem

Liability often turns on what someone knew or should have known. AI systems don't "know" things in the human sense. They have weights and activations, not beliefs.

Attributing knowledge to developers or deployers is possible but often fictional—they genuinely didn't know what the system would do in specific cases.

### The Control Problem

Liability assumes control—you're responsible because you could have done otherwise. But complex AI systems may not be controllable in the relevant sense.

You can choose whether to deploy, but once deployed, you may not be able to predict or prevent specific behaviors.

## Possible Futures

### The Liability Waiver Society

Widespread use of liability waivers makes AI deployment risk-free for deployers. Users bear all risk. Those harmed have no recourse.

This is already happening in consumer AI applications. The terms of service you didn't read assigned you all the risk.

### The Chilling Effect

Strict liability makes AI deployment so risky that beneficial applications are abandoned. Only the reckless deploy AI; the careful stay away.

Society loses the benefits of AI to avoid the harms.

### The Regulatory Capture

Large AI companies shape liability rules to their advantage. They can afford compliance costs that smaller competitors cannot. Liability becomes a barrier to entry.

### The Public Absorption

Government accepts that AI harms are unassignable and absorbs them as a public cost. Taxpayers compensate victims; deployers externalize costs.

This works if AI benefits are broadly distributed. It fails if benefits concentrate while harms diffuse.

### The New Social Contract

Society renegotiates the relationship between individual harm and collective benefit. Some harms are accepted as the cost of progress. New compensation mechanisms emerge that don't require assigning blame.

## Implications

The liability vacuum isn't a technical problem waiting for a technical solution. It reflects a fundamental mismatch between how AI works and how legal responsibility is structured.

Possible paths forward:

**Accept uncertainty**: Assign liability imperfectly, knowing that the assignment is somewhat arbitrary. This preserves the system's functions at the cost of some injustice.

**Restructure incentives**: Focus less on after-the-fact liability and more on before-the-fact safety requirements. Prevention over compensation.

**Collective mechanisms**: Move from individual liability to collective insurance, funds, and social safety nets. Accept that some risks are societal, not individual.

**Slow down**: Limit AI deployment until liability frameworks catch up. This has costs too—benefits delayed, competition lost.

None of these is fully satisfactory. The liability vacuum is a genuine dilemma, not a problem with a clean solution.

What's clear is that the current approach—pretending existing frameworks apply while they manifestly don't—is unstable. Something will give.

---

*This article explores the infrastructure implications of [Agency Multiplication](/articles/agency-multiplication). For related analysis, see [The Last Human Judge](/articles/last-human-judge) and [The Governance Fork](/articles/governance-fork).*
